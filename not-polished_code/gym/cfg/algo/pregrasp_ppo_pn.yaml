# 0708 work version
seed: -1

clip_observations: 5.0
clip_actions: 3.0
clip_relative_actions: 0.5
gen_pc: False

policy: # only works for MlpPolicy right now
  save_obs_path: None
  GAIL_debug: False
  pi_hid_sizes: [512, 512, 64]
  vf_hid_sizes: [512, 512, 64]
  activation: elu # can be elu, relu, selu, crelu, lrelu, tanh, sigmoid
  action_normalization: tanh
  use_dagger_activation: False
  actor_freeze: False
  feature_dim: 112
  network_type: New
  data_parallel: True
  use_orthogonal_init : True    ###trick 8
  use_pc: True
  backbone_type: None
  use_pretrain: True 
  ckpt: None
  seg_focal_loss: True
  rl_backbone_freeze: False
  concat_part_center: False
  task_meta:
    mask_dim: 4 # R, G, B, SEG,  #handle, part, gripper

  ###for dapg baseline###
  use_bc_pretrain: False

  use_dapg: False
  buffer_length: 1
  bc_epochs: 20

  mb_size: 400   #BC update minibatch
  joint_update: True
  update_which: "all"
  update_pn_interval: 10

  lambda0: 0.1
  lambda1: 0.99
  lambda0_: 0.01

  #####end dapg####

  use_expert: True
  disc_reward_perportion: 0.5
  max_batch: 100
  use_self_imitation: True
  demo_num: 5
  dagger_loss_supervise: 1
  dagger_loss_RL: 1
  dagger_loss_seg: 1
  dagger_loss_decay_proportion: 0.01
  learning_rate_dagger: 3.e-4
  use_domain_discriminator: False

  # use_boundingbox: True
  use_discriminator: False # use discriminator for addtional reward
  use_seg: False # use segmetation for addtional supervision gradient
  Spconv:
    class_path: perception.models.backbone.PointGroup
    ckpt_pth: None #ckpt/epoch_210_miou_70.98.ckpt
    device: None
    num_classes: 4
    in_channels: 6
    channels: [16, 64, 112] #[16, 32, 48, 64, 80, 96, 112]
    freeze: False
  SegSpconv:
    # class_path: perception.models.backbone.PointGroup
    # ckpt_pth: None
    # # device: None
    # num_classes: 4
    # in_channels: 6
    # channels_down: [16, 64, 112]
    # channels_up: [512, 256, 112]
    # freeze: False
    # block_repeat: 1
    class_path: perception.models.backbone.PointGroup
    ckpt_pth: None #ckpt/epoch_210_miou_70.98.ckpt
    device: None
    num_classes: 4
    in_channels: 6
    channels: [16,64,112] # [16,32,48,64,80,96,112] 
    freeze: False
    # class_path: perception.models.backbone.PointGroup
    # ckpt_pth: ckpt/epoch_508_miou_0.00.ckpt
    # device: None
    # num_classes: 4
    # in_channels: 6
    # channels: [16, 64, 112]
    # freeze: False
  
  freeze: False

  canonicalize: False
  canon_space: handle
  use_residual_traj: False
  

expert_policy: # only works for MlpPolicy right now
  pi_hid_sizes: [512, 512, 64]
  vf_hid_sizes: [512, 512, 64]
  activation: elu # can be elu, relu, selu, crelu, lrelu, tanh, sigmoid
  action_normalization: tanh
  feature_dim: 112
  use_pc: False
  network_type: New
  data_parallel: True
  use_orthogonal_init : True    ###trick 8
  expert_buffer_size: 2000
  action_clip: False

  task_meta: 
    state_dim: 56
    mask_dim: 4 # R, G, B, SEG,  #handle, part, gripper

  use_boundingbox: True
  canonicalize: True
  canon_space: handle
  use_residual_traj: False

  ###dapg baseline###
  use_dapg: False
  
  #use demo
  demo_path: /scratch/genghaoran/haoran/RL-Pose/PoseOrientedGym/assets/demo/expert_data_pos/pc_part_pregrasp_new
  #use_expert
  expert_path: /data2/formal_expert/pos/tanh_drawer_new_model_1180.tar

discriminator:
  hid_sizes: [128, 128, 32]
  activation: tanh
  use_bn: False
  use_dropout: True
  dropout_p: 0.5
  use_random_transitions: False

  task_meta: 
    state_dim: 56
  use_feature: True
  use_gt_obs: True

  lr: 0.0003
  disc_momentum: 0.2
  update_freq: 1
  weight_reg: 1.e-4
  use_grad_pen: true
  grad_pen_weight: 10

  rew_clip_magnitude: 5

learn:
  agent_name: franka
  test: False
  resume: 0
  save_interval: 50 # check for potential saves every this many iterations
  checkpoint: -1
  #checkpoint_path: /data2/ziming/RL-Pose/MyGym/logs/FrankaCabinet/ppo_1000021-8,mini=4_door0.05_reward0_seed9999/model_1600.tar
  eval_round: 5  # 1
  eval_freq: 50 # 500
  print_log: True

  # rollout params
  max_iterations: 20000

  # training params
  cliprange: 0.1  #!
  ent_coef: 0.01  #!
  nsteps: 200
  noptepochs: 8  #每次update多少个epoch !
  nminibatches: 16 # this is per agent,就是一共多少个mini batch  !

  optim_stepsize: 3.e-4 # 3e-4 is default for single agent training with constant schedule
  schedule: adaptive # could be adaptive or linear or fixed !
  desired_kl: 0.01 ### trick 0, use KL or not !
  lr_upper: 1e-3
  lr_lower: 1e-7
  gamma: 0.99
  lam: 0.95
  init_noise_std: 1

  log_interval: 1
  asymmetric: False

  use_adv_norm : True        ### trick 1 
  adv_norm_epsilon: 1.e-8
  use_state_norm: False     ### trick 2 
  use_reward_norm: False        ### trick 3
  use_reward_scaling: False   ### trick 4
  learning_rate_decay : True   ### trick 6 !
  use_grad_clip : True          ###trick 7 !
  max_grad_norm: 0.5                          #!
  adam_epsilon : 1.e-5           ### trick 9 !

###heuristic
useGtBbox: True
eval:
  iterations: 1